from __future__ import annotations
import re
from typing import List, Union, Optional

from langchain_text_splitters import RecursiveCharacterTextSplitter
from app.domain.page_element import PageElement
from app.domain.page_chunk   import PageChunk          # â˜… NEW
from app.dto.summary_dto import SummaryRequestDTO

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•˜ì´í¼ íŒŒë¼ë¯¸í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# íŠœí† ë¦¬ì–¼ ìƒì„±ì„ ìœ„í•´ ì²­í¬ í¬ê¸°ë¥¼ í™•ëŒ€
# - ë” í° ì²­í¬ = ë” í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ = ë” ë‚˜ì€ ì„¤ëª… ìƒì„±
_MAX, _OVF, _OVL = 2_000, 2_500, 300
sent_split = RecursiveCharacterTextSplitter(
    chunk_size=_MAX,
    chunk_overlap=_OVL,
    separators=["\n\n", ". ", "! ", "? ", "\n"],  # ë¶„ë¦¬ì ìš°ì„ ìˆœìœ„ ê°œì„ 
)

_MD_HEADER = re.compile(r"^#{1,6}\s+.+")
# bullets ê·¸ëŒ€ë¡œ ìœ ì§€ (ì‹œì¥ì  í•„ìš”)
_BULLET = re.compile(r"^(\s*[\u2022\u2023\u25CF\-\*])|^\s*\d+\.\s+")
_PAR_BR = re.compile(r"\n{2,}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SemanticChunker:
    """
    PageElement ë¦¬ìŠ¤íŠ¸ â†’ semantic chunk ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜.

    Parameters
    ----------
    return_pagechunk : True ì´ë©´ PageChunk ê°ì²´,
                       False ì´ë©´ plain str ì„ ëŒë ¤ì¤€ë‹¤.
    
    ê°œì„  ì‚¬í•­:
    - í˜ì´ì§€ ê²½ê³„ ì²˜ë¦¬ ì™„í™” (ì‘ì€ ë²„í¼ëŠ” ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ì–´ì§)
    - ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ë°˜ ì²­í‚¹ ê°œì„ 
    - ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì—°ê´€ì„± ë³´ì¡´
    - ìƒì„¸í•œ ë””ë²„ê¹… ë¡œê·¸
    """

    def __init__(self, max_chunk_size: int = 2000, overflow_threshold: int = 2500, overlap: int = 300):
        self.max_chunk_size = max_chunk_size
        self.overflow_threshold = overflow_threshold
        self.overlap = overlap
        self.min_chunk_size = 400  # ë„ˆë¬´ ì‘ì€ ì²­í¬ ë°©ì§€ (ê¸°ì¡´ 300ì—ì„œ ì¦ê°€)

    def group(
        self,
        els: List[PageElement],
        *,
        return_pagechunk: bool = False
    ) -> List[Union[str, PageChunk]]:
        print(f"[SemanticChunker] ì²­í‚¹ ì‹œì‘: {len(els)}ê°œ ìš”ì†Œ", flush=True)
        blocks, buf, figs = [], [], []
        chunk_count = 0

        def flush(page_no: int, reason: str = ""):
            """ë²„í¼ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ë°€ì–´ ë„£ëŠ”ë‹¤."""
            nonlocal chunk_count
            if not buf:
                return

            joined = " ".join(buf).strip()
            buf_size = len(joined)
            
            # ë””ë²„ê¹…: flush ì´ìœ  ì¶œë ¥
            if reason:
                print(f"[SemanticChunker] ğŸ”„ Flush íŠ¸ë¦¬ê±°: {reason} (í˜ì´ì§€ {page_no}, ë²„í¼ í¬ê¸°: {buf_size}ì)", flush=True)
            
            # Overflow ì²˜ë¦¬: overflow_thresholdë¥¼ ì´ˆê³¼í•˜ë©´ RecursiveCharacterTextSplitterë¡œ ë¶„í• 
            texts  = (
                sent_split.split_text(joined)
                if buf_size > self.overflow_threshold
                else [joined]
            )
            
            # ë””ë²„ê¹…: ë¶„í•  ê²°ê³¼
            if len(texts) > 1:
                print(f"[SemanticChunker]   â†’ Overflowë¡œ {len(texts)}ê°œ ì„œë¸Œì²­í¬ë¡œ ë¶„í• ", flush=True)

            if return_pagechunk:
                for i, t in enumerate(texts):
                    chunk_count += 1
                    chunk = PageChunk(page=page_no, text=t, figs=list(figs))
                    blocks.append(chunk)
                    # ë””ë²„ê¹…: ì²­í¬ ìƒì„± ë¡œê·¸
                    img_ids = [img_id for img_id, _ in figs]
                    print(
                        f"[SemanticChunker]   âœ… ì²­í¬ {chunk_count} ìƒì„±: "
                        f"í˜ì´ì§€ {page_no}, {len(t)}ì, "
                        f"ì´ë¯¸ì§€ {len(img_ids)}ê°œ {img_ids if img_ids else ''}",
                        flush=True
                    )
            else:
                blocks.extend(texts)
                chunk_count += len(texts)

            buf.clear()
            figs.clear()

        last_page = -1
        buf_has_header = False  # ë²„í¼ì— í—¤ë”ê°€ ìˆëŠ”ì§€ ì¶”ì 

        for idx, el in enumerate(els):
            current_buf_size = sum(len(x) for x in buf)
            
            # í˜ì´ì§€ê°€ ë°”ë€ŒëŠ” ê²½ìš° ì²˜ë¦¬ ê°œì„ 
            if el.page_no != last_page:
                # ë²„í¼ê°€ ì¶©ë¶„íˆ í¬ë©´ flush (ì˜ë¯¸ ìˆëŠ” ì²­í¬)
                # ë²„í¼ê°€ ì‘ìœ¼ë©´ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ì–´ê° (ë¬¸ë§¥ ë³´ì¡´)
                if current_buf_size > self.min_chunk_size:
                    flush(last_page, f"í˜ì´ì§€ ë³€ê²½ ({last_page} â†’ {el.page_no})")
                elif buf and current_buf_size > 0:
                    print(
                        f"[SemanticChunker] ğŸ“‹ í˜ì´ì§€ {last_page} â†’ {el.page_no}: "
                        f"ë²„í¼ {current_buf_size}ìëŠ” ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ì–´ì§ (ë¬¸ë§¥ ë³´ì¡´)",
                        flush=True
                    )
                last_page = el.page_no
                buf_has_header = False

            if el.kind == "text":
                for p in _PAR_BR.split(el.content):
                    p = p.strip()
                    if not p:
                        continue
                    
                    # ë§ˆí¬ë‹¤ìš´ í—¤ë” ì²˜ë¦¬ ê°œì„ 
                    if _MD_HEADER.match(p):
                        # ë²„í¼ì— ì´ë¯¸ ë‚´ìš©ì´ ìˆìœ¼ë©´ flush (í—¤ë”ëŠ” ìƒˆ ì²­í¬ì˜ ì‹œì‘)
                        if buf and not buf_has_header:
                            flush(el.page_no, f"ë§ˆí¬ë‹¤ìš´ í—¤ë” ë°œê²¬: '{p[:50]}...'")
                        buf.append(p)
                        buf_has_header = True
                    elif _BULLET.match(p):
                        buf.append(p)
                    else:
                        buf.append(p)
                        
            else:  # figure / table / graph
                # ì´ë¯¸ì§€ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ í…ìŠ¤íŠ¸ ë²„í¼ì— ì¶”ê°€
                buf.append(f"[{el.id}]")
                
                # ì´ë¯¸ì§€ ì •ë³´ ìˆ˜ì§‘
                content = el.content if isinstance(el.content, str) else "image_data"
                figs.append((el.id, content))
                
                print(f"[SemanticChunker] ğŸ–¼ï¸ ì´ë¯¸ì§€ ì¶”ê°€: {el.id} (í˜ì´ì§€ {el.page_no})", flush=True)

            # Overflow ì²´í¬ ê°œì„ : overflow_threshold ì‚¬ìš©
            current_buf_size = sum(len(x) for x in buf)
            if current_buf_size > self.overflow_threshold:
                flush(el.page_no, f"ë²„í¼ ì˜¤ë²„í”Œë¡œìš° ({current_buf_size} > {self.overflow_threshold})")
                buf_has_header = False

        # ë§ˆì§€ë§‰ ë²„í¼ ì²˜ë¦¬
        if buf:
            flush(last_page, "ë§ˆì§€ë§‰ ë²„í¼")
            
        print(f"[SemanticChunker] ì²­í‚¹ ì™„ë£Œ: {len(blocks)}ê°œ ì²­í¬ ìƒì„±", flush=True)
        return blocks

