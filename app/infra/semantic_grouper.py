"""
semantic_grouper.py
------------------
ê¸°ì¡´ ë²¡í„°DB ì¸í”„ë¼ë¥¼ í™œìš©í•˜ì—¬ PageChunkë“¤ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”í•˜ëŠ” ì»´í¬ë„ŒíŠ¸.
segment.pyì˜ ë¡œì§ì„ ì°¸ê³ í•˜ë˜, PageChunk íŠ¹ì„±ì— ë§ê²Œ ìµœì í™”í–ˆë‹¤.
"""

from __future__ import annotations
from typing import List, Optional
import numpy as np
from scipy.spatial.distance import cosine

from app.domain.page_chunk import PageChunk
from app.domain.interfaces import SemanticGrouperIF
from app.vectordb.vector_db import get_vector_db


class SemanticGrouper(SemanticGrouperIF):
    """ë²¡í„°DB ê¸°ë°˜ ì˜ë¯¸ ë‹¨ìœ„ ì²­í¬ ê·¸ë£¹í™”ê¸°"""
    
    def __init__(self):
        """ë²¡í„°DB ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•œë‹¤."""
        self.vdb = get_vector_db()
        self.sim_threshold = 0.75  # ê¸°ì¡´ 0.78ì—ì„œ ì™„í™” (ë„ˆë¬´ ì—„ê²©í•˜ë©´ ì‘ì€ ê·¸ë£¹ ì¦ê°€)
        self.max_gap_pages = 2     # ê¸°ì¡´ 1ì—ì„œ 2ë¡œ í™•ëŒ€ (ê´€ë ¨ ë‚´ìš©ì´ ì—¬ëŸ¬ í˜ì´ì§€ì— ê±¸ì¹  ìˆ˜ ìˆìŒ)
        self.max_group_size = 5    # ê¸°ì¡´ 3ì—ì„œ 5ë¡œ ì¦ê°€ (ë” í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ)
        self.min_group_size = 2    # ìµœì†Œ ê·¸ë£¹ í¬ê¸° (ë‹¨ì¼ ì²­í¬ ê·¸ë£¹ ë°©ì§€)
    
    def group_chunks(self, chunks: List[PageChunk]) -> List[List[PageChunk]]:
        """
        PageChunkë“¤ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”í•œë‹¤.
        
        Args:
            chunks: ê·¸ë£¹í™”í•  PageChunk ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ê·¸ë£¹í™”ëœ PageChunk ë¦¬ìŠ¤íŠ¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        if not chunks:
            return []
        
        # ì„ë² ë”© ìƒì„± (ë²¡í„°DBì˜ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©)
        embeddings = self._get_embeddings(chunks)
        
        # segment.py ë°©ì‹ìœ¼ë¡œ ê·¸ë£¹í™”
        groups = self._group_by_similarity(chunks, embeddings)
        
        # í›„ì²˜ë¦¬: ë„ˆë¬´ ì‘ì€ ê·¸ë£¹ ë³‘í•©
        groups = self._merge_small_groups(groups)
        
        return groups
    
    def _get_embeddings(self, chunks: List[PageChunk]) -> List[np.ndarray]:
        """ì²­í¬ë“¤ì˜ ì„ë² ë”©ì„ ìƒì„±í•œë‹¤."""
        if not chunks:
            return []
            
        texts = [chunk.text for chunk in chunks]
        
        # ë²¡í„°DBì˜ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
        embeddings = []
        for i, text in enumerate(texts):
            try:
                # ë²¡í„°DBì˜ ì„ë² ë”© í•¨ìˆ˜ ì‚¬ìš©
                embedding = self.vdb.embeddings.embed_query(text)
                embeddings.append(np.array(embedding))
            except Exception as e:
                print(f"[SemanticGrouper] ì²­í¬ {i} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}", flush=True)
                # ì‹¤íŒ¨ ì‹œ 0 ë²¡í„°ë¡œ ëŒ€ì²´
                embeddings.append(np.zeros(384))  # ê¸°ë³¸ ì°¨ì›
        
        return embeddings
    
    def _group_by_similarity(self, chunks: List[PageChunk], embeddings: List[np.ndarray]) -> List[List[PageChunk]]:
        """
        segment.py ë°©ì‹ì„ ì°¸ê³ í•˜ì—¬ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ì²­í¬ë“¤ì„ ê·¸ë£¹í™”í•œë‹¤.
        """
        if not chunks:
            return []
        
        groups = []
        current_group = [chunks[0]]
        current_embeddings = [embeddings[0]]
        centroid = embeddings[0]  # í˜„ì¬ ê·¸ë£¹ì˜ ì¤‘ì‹¬ë²¡í„°
        
        print(f"[SemanticGrouper] ê·¸ë£¹í™” ì‹œì‘: {len(chunks)}ê°œ ì²­í¬", flush=True)
        print(f"[SemanticGrouper] ì„¤ì •: threshold={self.sim_threshold}, max_gap={self.max_gap_pages}, max_size={self.max_group_size}", flush=True)
        
        for i in range(1, len(chunks)):
            chunk = chunks[i]
            embedding = embeddings[i]
            
            # í˜ì´ì§€ ê°„ê²© ê³„ì‚°
            gap = chunk.page - current_group[-1].page
            
            # ìœ ì‚¬ë„ ê³„ì‚° (segment.pyì™€ ë™ì¼í•œ ë°©ì‹)
            sim = 1 - cosine(centroid, embedding)
            
            # ë””ë²„ê¹…: ê·¸ë£¹í™” ê²°ì • ë¡œê·¸
            chunk_preview = chunk.text[:80] + "..." if len(chunk.text) > 80 else chunk.text
            decision_info = (
                f"ì²­í¬ {i+1}(í˜ì´ì§€ {chunk.page}): ìœ ì‚¬ë„={sim:.3f}, "
                f"í˜ì´ì§€ê°„ê²©={gap}, ê·¸ë£¹í¬ê¸°={len(current_group)}, "
                f"ë¯¸ë¦¬ë³´ê¸°='{chunk_preview}'"
            )
            
            # ê·¸ë£¹í™” ì¡°ê±´ í™•ì¸
            if (sim >= self.sim_threshold and 
                gap <= self.max_gap_pages and
                len(current_group) < self.max_group_size):
                # ê°™ì€ ê·¸ë£¹ì— ì¶”ê°€
                print(f"[SemanticGrouper] âœ… {decision_info} â†’ ê·¸ë£¹ {len(groups)+1}ì— ì¶”ê°€", flush=True)
                current_group.append(chunk)
                current_embeddings.append(embedding)
                # ì¤‘ì‹¬ë²¡í„° ì—…ë°ì´íŠ¸ (ì „ì²´ ê·¸ë£¹ì˜ í‰ê· ìœ¼ë¡œ ì •í™•í•˜ê²Œ ê³„ì‚°)
                centroid = np.mean(current_embeddings, axis=0)
            else:
                # ìƒˆ ê·¸ë£¹ ì‹œì‘
                reason = []
                if sim < self.sim_threshold:
                    reason.append(f"ìœ ì‚¬ë„ë‚®ìŒ({sim:.3f}<{self.sim_threshold})")
                if gap > self.max_gap_pages:
                    reason.append(f"í˜ì´ì§€ê°„ê²©í¼({gap}>{self.max_gap_pages})")
                if len(current_group) >= self.max_group_size:
                    reason.append(f"ê·¸ë£¹í¬ê¸°ì´ˆê³¼({len(current_group)}>={self.max_group_size})")
                
                print(f"[SemanticGrouper] ğŸ”„ {decision_info} â†’ ìƒˆ ê·¸ë£¹ ì‹œì‘ (ì´ìœ : {', '.join(reason)})", flush=True)
                groups.append(current_group)
                current_group = [chunk]
                current_embeddings = [embedding]
                centroid = embedding
        
        # ë§ˆì§€ë§‰ ê·¸ë£¹ ì¶”ê°€
        if current_group:
            groups.append(current_group)
        
        print(f"[SemanticGrouper] ì´ˆê¸° ê·¸ë£¹í™” ì™„ë£Œ: {len(groups)}ê°œ ê·¸ë£¹ ìƒì„±", flush=True)
        for i, grp in enumerate(groups, 1):
            pages = [c.page for c in grp]
            print(f"[SemanticGrouper]   ê·¸ë£¹ {i}: {len(grp)}ê°œ ì²­í¬, í˜ì´ì§€ {min(pages)}-{max(pages)}", flush=True)
        
        return groups
    
    def _merge_small_groups(self, groups: List[List[PageChunk]]) -> List[List[PageChunk]]:
        """
        ë„ˆë¬´ ì‘ì€ ê·¸ë£¹ì„ ì¸ì ‘ ê·¸ë£¹ê³¼ ë³‘í•©í•œë‹¤.
        ë‹¨ì¼ ì²­í¬ ê·¸ë£¹ì´ë‚˜ min_group_sizeë³´ë‹¤ ì‘ì€ ê·¸ë£¹ì„ ì²˜ë¦¬í•œë‹¤.
        """
        if not groups or len(groups) == 1:
            return groups
        
        print(f"[SemanticGrouper] ì†Œê·¸ë£¹ ë³‘í•© ì‹œì‘: {len(groups)}ê°œ ê·¸ë£¹", flush=True)
        
        merged_groups = []
        i = 0
        
        while i < len(groups):
            current_group = groups[i]
            
            # í˜„ì¬ ê·¸ë£¹ì´ ìµœì†Œ í¬ê¸° ë¯¸ë§Œì¸ ê²½ìš°
            if len(current_group) < self.min_group_size:
                # ë‹¤ìŒ ê·¸ë£¹ê³¼ ë³‘í•© ì‹œë„
                if i + 1 < len(groups):
                    next_group = groups[i + 1]
                    # ë³‘í•©í•´ë„ max_group_sizeë¥¼ ì´ˆê³¼í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ ë³‘í•©
                    if len(current_group) + len(next_group) <= self.max_group_size * 1.5:
                        merged = current_group + next_group
                        pages = [c.page for c in merged]
                        print(
                            f"[SemanticGrouper] ğŸ”— ê·¸ë£¹ {i+1}({len(current_group)}ê°œ)ì™€ "
                            f"ê·¸ë£¹ {i+2}({len(next_group)}ê°œ) ë³‘í•© â†’ "
                            f"{len(merged)}ê°œ ì²­í¬ (í˜ì´ì§€ {min(pages)}-{max(pages)})",
                            flush=True
                        )
                        merged_groups.append(merged)
                        i += 2  # ë‘ ê·¸ë£¹ì„ ëª¨ë‘ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ +2
                        continue
                
                # ë³‘í•©í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, ì´ì „ ê·¸ë£¹ê³¼ ë³‘í•© ì‹œë„
                if merged_groups and len(merged_groups[-1]) + len(current_group) <= self.max_group_size * 1.5:
                    prev_group = merged_groups[-1]
                    merged = prev_group + current_group
                    pages = [c.page for c in merged]
                    print(
                        f"[SemanticGrouper] ğŸ”— ê·¸ë£¹ {i+1}({len(current_group)}ê°œ)ì„ "
                        f"ì´ì „ ê·¸ë£¹({len(prev_group)}ê°œ)ì— ë³‘í•© â†’ "
                        f"{len(merged)}ê°œ ì²­í¬ (í˜ì´ì§€ {min(pages)}-{max(pages)})",
                        flush=True
                    )
                    merged_groups[-1] = merged
                else:
                    # ë³‘í•© ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ê·¸ëŒ€ë¡œ ì¶”ê°€ (ë§ˆì§€ë§‰ ê·¸ë£¹ì´ê±°ë‚˜ ë³‘í•© ì‹œ ë„ˆë¬´ í° ê²½ìš°)
                    print(
                        f"[SemanticGrouper] âš ï¸ ê·¸ë£¹ {i+1}({len(current_group)}ê°œ)ì€ "
                        f"ë³‘í•© ë¶ˆê°€ëŠ¥í•˜ì—¬ ê·¸ëŒ€ë¡œ ìœ ì§€",
                        flush=True
                    )
                    merged_groups.append(current_group)
            else:
                # ìµœì†Œ í¬ê¸°ë¥¼ ë§Œì¡±í•˜ëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì¶”ê°€
                merged_groups.append(current_group)
            
            i += 1
        
        print(f"[SemanticGrouper] ë³‘í•© ì™„ë£Œ: {len(merged_groups)}ê°œ ê·¸ë£¹ (ë³‘í•© ì „: {len(groups)}ê°œ)", flush=True)
        for i, grp in enumerate(merged_groups, 1):
            pages = [c.page for c in grp]
            print(f"[SemanticGrouper]   ìµœì¢… ê·¸ë£¹ {i}: {len(grp)}ê°œ ì²­í¬, í˜ì´ì§€ {min(pages)}-{max(pages)}", flush=True)
        
        return merged_groups
    
    def set_similarity_threshold(self, threshold: float):
        """ìœ ì‚¬ë„ ì„ê³„ê°’ì„ ì„¤ì •í•œë‹¤."""
        self.sim_threshold = threshold
    
    def set_max_gap_pages(self, max_gap: int):
        """ìµœëŒ€ í˜ì´ì§€ ê°„ê²©ì„ ì„¤ì •í•œë‹¤."""
        self.max_gap_pages = max_gap
    
    def set_max_group_size(self, max_size: int):
        """ìµœëŒ€ ê·¸ë£¹ í¬ê¸°ë¥¼ ì„¤ì •í•œë‹¤."""
        self.max_group_size = max_size
    
    def set_min_group_size(self, min_size: int):
        """ìµœì†Œ ê·¸ë£¹ í¬ê¸°ë¥¼ ì„¤ì •í•œë‹¤."""
        self.min_group_size = min_size
    
    def get_grouping_stats(self, chunks: List[PageChunk]) -> dict:
        """ê·¸ë£¹í™” í†µê³„ë¥¼ ë°˜í™˜í•œë‹¤."""
        if not chunks:
            return {"total_chunks": 0, "total_groups": 0, "avg_group_size": 0}
        
        groups = self.group_chunks(chunks)
        
        return {
            "total_chunks": len(chunks),
            "total_groups": len(groups),
            "avg_group_size": len(chunks) / len(groups) if groups else 0,
            "group_sizes": [len(group) for group in groups]
        }


# ì‹±ê¸€í„´ ì¸ìŠ¤í„´ìŠ¤
_semantic_grouper = SemanticGrouper()

def get_semantic_grouper() -> SemanticGrouper:
    """SemanticGrouper ì‹±ê¸€í„´ì„ ë°˜í™˜í•œë‹¤."""
    return _semantic_grouper 